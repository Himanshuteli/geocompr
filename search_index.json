[
["index.html", "Geocomputation with R Welcome Prerequisites", " Geocomputation with R Robin Lovelace Jakub Nowosad 2017-05-20 Welcome Welcome to the geocompr website, the online home of our forthcoming book with CRC Press. Inspired by the bookdown R package and book-hosting website, we are developing this book in the open. We encourage contributions. bookdown makes editing a book as easy as editing a wiki. To do so, just click on the ‘edit me’ icon highlighted in the image below. Which-ever chapter you are looking at, this will take you to the source R Markdown file hosted on GitHub. If you have a GitHub account, you’ll be able to make changes there and submit a pull request. If you do not, it’s time to sign-up! To raise an issue about the book’s content (e.g. code not running) or make a feature request, check-out the issue tracker. Prerequisites To reproduce the book, first install it as a github package: if(!require(devtools)) { install.packages(&quot;devtools&quot;) } devtools::install_github(&quot;robinlovelace/geocompr&quot;) This should install all the necessary packages to run the code. To build the book locally, clone or download the repo and run the following line from the project’s root directory: bookdown::render_book(&quot;index.Rmd&quot;) For further details see the book’s GitHub page at Robinlovelace/geocompr. "],
["intro.html", "1 Introduction 1.1 Why Geocomputation with R? 1.2 R’s spatial ecosystem", " 1 Introduction This book exists at the intersection between Geography and R (Figure 1.1). Suggested titles were Geography with R and R for GIS, each of which has advantages. The former conveys the message that it’s not just about spatial data: non-spatial attribute data are inevitably interwoven with geometry data, and Geography is about more than where something is on the map. The latter option communicates that this is a book about using R as a Geographic Information System (GIS), to perform spatial operations on geographic data (Bivand, Pebesma, and Gómez-Rubio 2013). However, the term GIS has some negative connotations (see Table 1.1) and fails to communicate one of R’s greatest strengths: its unparalleled ability to seamlessly switch between geographic and non-geographic data processing, modeling and visualization tasks. Figure 1.1: Venn diagram of the intersection between Geography and R. The title was chosen because this book is about more than routine spatial data processing, something that is well captured by the term geocomputation. What defines geocomputation as a development on previous work is the “creative and experimental use of GIS that it entails” (Longley et al. 1998). With the right know-how, geographic data can be used in ways that would have been unthinkable to early practitioners of GIS. Another advantage of geocomputation that it conveys the centrality of the concept of algorithms (which can be implemented in R functions) to advanced geographical research. Geocomputation is a relatively young field1 but methods published under the geocomputational banner have influenced the direction of geographical research, as we will see in subsequent chapters. Algorithms are powerful tools that can become highly complex. However, all algorithms are composed of smaller, often modular parts. By teaching these foundations we aim to empower you. Creating your own solutions to geographic data problems can feel breaking free from the metaphorical ‘glass ceiling’ imposed by GUI-based proprietary geographic information systems (see Table 1.1 for a definition of GUI). While embracing recent developments in the field, we also wanted to pay respects the wider field of Geography, and its 2000 year history (Roller 2010), of which geocomputation is a part. Geography has played an important role in explaining and influencing humanity’s relationship with the natural world and this book aims to be a part of the ‘Geographic tradition’. Geocomputation in this book therefore means more than simply analyzing geographic data on a computer. It’s about trying to make research which involves making geographic data more reproducible, scientific and socially beneficial. This book is also part of the movement towards Geographic Data Science (GDS) which differs from GIS in several ways, some of which are outlined in Table 1.1. Inspired by the emphasis of GDS research on reproducibility, this book aims teach how to do geocomputation rather than just think about it. Table 1.1: Differences in emphasis between the fields of Geographic Information Systems (GIS) and Geographic Data Science (GDS). Attribute GIS GDS Home disciplines Geography Geography,Computing,Statistics Software focus Graphical User Interface Code Reproduciblility Minimal Maximal 1.1 Why Geocomputation with R? In this book we treat R as a ‘tool for the trade’. Early geographers used a variety of tools including rulers, compasses and sextants to advance knowledge about the world. It is important to remember that while R is a powerful tool, especially when interfaced with other software such as GDAL and PostGIS, other tools may be better for certain tasks. R is characterised by its flexibility, enabling geographical software developers to extend it in multiple ways. A good example of this is support for generating interactive maps thanks to leaflet and extended by tmap and mapview. These packages help overcome the criticism that R has “limited interactive [plotting] facilities” (Bivand, Pebesma, and Gómez-Rubio 2013). This is no longer true, as illustrated by the code below which generates Figure 1.2. library(leaflet) popup = c(&quot;Robin&quot;, &quot;Jakub&quot;) leaflet() %&gt;% addProviderTiles(&quot;NASAGIBS.ViirsEarthAtNight2012&quot;) %&gt;% addAwesomeMarkers(lng = c(-3, 23), lat = c(52, 53), popup = popup) Figure 1.2: World at night imagery from NASA overlaid by the authors’ approximate home locations to illustrate interactive mapping with R. It would be difficult to produce Figure 1.2 with only 4 lines of code in another language, let alone embed the results in an interactive html page (the interactive version can be viewed at bookdown.org/robinlovelace/geocompr/intro.html), illustrating R’s flexibility and power. R provides a stable language that should behave consistently across multiple computers, free of charge. The use of R code therefore enables teaching geocomputation with reference to reproducible examples such as that provided in 1.2 rather than abstract concepts. But before cracking-on with the action, a few introductory remarks are needed to explain the approach taken here and provide context. 1.2 R’s spatial ecosystem The most important recent evolution in R’s spatial ecosystem has without doubt been support for simple features thanks to the sf package, introduced below (see Chapter 2 for a detailed account of the sf class system). References "],
["spatial-class.html", "2 Spatial classes Prerequisites 2.1 An introduction to Simple Features 2.2 Why Simple Features? 2.3 Basic map making 2.4 Vector data 2.5 Raster data", " 2 Spatial classes Prerequisites 2.1 An introduction to Simple Features Simple Features is an open standard data model developed and endorsed by the Open Geospatial Consortium (OGC) to describe how features with geographical and non-geographical features should be represented. It is a hierarchical data model that simplifies geographic data by condensing the complex range of possible geographic forms (e.g., line, point, polygon, multipolygon forms) into a single geometry class. The R implementation of Simple Features is provided by the sf package (E. Pebesma 2017). sf incorporates the functionality of the 3 main packages of the sp paradigm (sp (E. Pebesma and Bivand 2016) for the class system, rgdal (Bivand, Keitt, and Rowlingson 2016) for reading and writing data, rgeos (Bivand and Rundel 2017) for spatial operations undertaken by GEOS) in a single, cohesive whole. This is well-documented in sf’s vignettes: vignette(&quot;sf1&quot;) # for an introduction to the package vignette(&quot;sf2&quot;) # for reading, writing and converting Simple Features vignette(&quot;sf3&quot;) # for manipulating Simple Features As the first vignette explains, simple feature objects in R are stored in a data frame, with geographical data occupying special column, a ‘list-column’. This column is usually named ‘geom’ or ‘geometry’. Let’s see how simple feature in R work, with reference to world boundary data from the spData package: library(sf) #&gt; Linking to GEOS 3.5.0, GDAL 2.1.0, proj.4 4.8.0 # devtools::install_github(&quot;nowosad/spData&quot;) f = system.file(&quot;shapes/wrld.shp&quot;, package = &quot;spData&quot;) world = st_read(f) This has loaded an object that is simultaneously of class data.frame and sf: class(world) #&gt; [1] &quot;sf&quot; &quot;data.frame&quot; The output of the preceding command shows that objects with class sf are also data frames. Thus, they can be treated like regular data.frame, making life easy if you are already used to working with data frames. Let’s look the first 2 rows and 3 columns of this object. The output shows 2 major differences compared with a regular data.frame: the inclusion of additional geographical data (geometry type, dimension, bbox and CRS information - epsg (SRID), proj4string), and the presence of final geometry column: world[1:2, 1:3] #&gt; Simple feature collection with 2 features and 3 fields #&gt; geometry type: MULTIPOLYGON #&gt; dimension: XY #&gt; bbox: xmin: 11.6401 ymin: -17.93064 xmax: 75.15803 ymax: 38.48628 #&gt; epsg (SRID): 4326 #&gt; proj4string: +proj=longlat +datum=WGS84 +no_defs #&gt; iso_a2 name_long continent geometry #&gt; 1 AF Afghanistan Asia MULTIPOLYGON(((61.210817091... #&gt; 2 AO Angola Africa MULTIPOLYGON(((16.326528354... All this may seem rather complex, especially for a class system that is supposed to be simple. However, there are good reasons for organizing things this way and using sf. 2.1.1 Exercises What does the summary of the geometry column tell us about the world dataset, in terms of: The geometry type? How many countries there are? The coordinate reference system (CRS)? 2.2 Why Simple Features? There are many advantages of sf over sp, including: Faster reading and writing of data (more than 10 times faster in some cases) Better plotting performance sf objects can be treated as dataframes in most operations sf functions can be combined using %&gt;% operator and works well with the tidyverse collection of R packages sf function names are relatively consistent and intuitive (all begin with st_) compared with the function names and syntax of the sp, rgdal and rgeos packages that it supersedes. A broader advantage is that simple features are so well supported by other software products, not least PostGIS, which has heavily influenced the design of sf. A disadvantage you should be aware of, however, is that sf is not feature complete and that it continues to evolve. The transition from sp to sf will likely take many years, and many spatial packages may never switch. Even if you discover spatial data with R through the sf package, it is still worth at least being aware of sp classes, even if you rarely use them for everyday geospatial tasks. Fortunately it is easy to translate between sp and sf using the as() function: library(sp) world_sp = as(object = world, Class = &quot;Spatial&quot;) 2.3 Basic map making Basic maps in sf can be created quickly with the base plot() function. Unlike sp, however, sf by default creates a faceted plot, one sub-plot for each variable, as illustrated in the left-hand image in Figure 2.1. plot(world) plot(world[&quot;pop&quot;]) Figure 2.1: Plotting with sf, with multiple variables (left) and a single variable (right). As with sp, you can add layers to your maps created with plot(), with the argument add = TRUE2. However, this only works if the initial plot has only 1 layer (result not shown): plot(world[&quot;pop&quot;]) china = world[world$name_long == &quot;China&quot;, ] plot(china, add = TRUE, col = &quot;red&quot;) This can be very useful when quickly checking the geographic correspondence between two or more layers. These plots work well for gaining a quick understanding of the data with few lines of code. For more advanced map making we recommend using a dedicated visualisation package such as tmap, ggplot2, mapview, or leaflet. 2.3.1 Challenge Using sf’s plot() command, create a map of Nigeria in context, like the one presented in figure 2.2. Hint: this used the lwd, main and col arguments of plot(). Bonus: make the country boundaries a dotted grey line. Hint: border is an additional argument of plot() for sf objects. Figure 2.2: Map of Nigeria in context illustrating sf’s plotting capabilities 2.3.2 Further work sf makes R data objects more closely aligned to the data model used in GDAL and GEOS, in theory making spatial data operations faster. The work here provides a taster of the way that sf operates but there is much more to learn (see Chapter 4). There is a wealth of information that is available in the package’s vignettes: these are highly recommended. As a final exercise, we’ll see how to do a spatial overlay in sf by first converting the countries of the world into centroids and then subsetting those in Africa: world_centroids = st_centroid(world) #&gt; Warning in st_centroid.sfc(st_geometry(x)): st_centroid does not give #&gt; correct centroids for longitude/latitude data plot(world_centroids[1]) africa_centroids = world_centroids[africa,] #&gt; although coordinates are longitude/latitude, it is assumed that they are planar plot(africa_centroids, add = TRUE, cex = 2) Figure 2.3: Centroids in Africa Note: another way of acheiving the same result is with a GEOS function for identifying spatial overlay: sel_africa = st_covered_by(world_centroids, africa, sparse = FALSE) #&gt; although coordinates are longitude/latitude, it is assumed that they are planar summary(sel_africa) #&gt; V1 V2 V3 V4 #&gt; Mode :logical Mode :logical Mode :logical Mode :logical #&gt; FALSE:176 FALSE:176 FALSE:176 FALSE:176 #&gt; TRUE :1 TRUE :1 TRUE :1 TRUE :1 #&gt; V5 V6 V7 V8 #&gt; Mode :logical Mode :logical Mode :logical Mode :logical #&gt; FALSE:176 FALSE:176 FALSE:176 FALSE:176 #&gt; TRUE :1 TRUE :1 TRUE :1 TRUE :1 #&gt; V9 V10 V11 V12 #&gt; Mode :logical Mode :logical Mode :logical Mode :logical #&gt; FALSE:176 FALSE:176 FALSE:176 FALSE:176 #&gt; TRUE :1 TRUE :1 TRUE :1 TRUE :1 #&gt; V13 V14 V15 V16 #&gt; Mode :logical Mode :logical Mode :logical Mode :logical #&gt; FALSE:176 FALSE:176 FALSE:176 FALSE:176 #&gt; TRUE :1 TRUE :1 TRUE :1 TRUE :1 #&gt; V17 V18 V19 V20 #&gt; Mode :logical Mode :logical Mode :logical Mode :logical #&gt; FALSE:176 FALSE:176 FALSE:176 FALSE:176 #&gt; TRUE :1 TRUE :1 TRUE :1 TRUE :1 #&gt; V21 V22 V23 V24 #&gt; Mode :logical Mode :logical Mode :logical Mode :logical #&gt; FALSE:176 FALSE:176 FALSE:176 FALSE:176 #&gt; TRUE :1 TRUE :1 TRUE :1 TRUE :1 #&gt; V25 V26 V27 V28 #&gt; Mode :logical Mode :logical Mode :logical Mode :logical #&gt; FALSE:176 FALSE:176 FALSE:176 FALSE:176 #&gt; TRUE :1 TRUE :1 TRUE :1 TRUE :1 #&gt; V29 V30 V31 V32 #&gt; Mode :logical Mode :logical Mode :logical Mode :logical #&gt; FALSE:176 FALSE:176 FALSE:176 FALSE:176 #&gt; TRUE :1 TRUE :1 TRUE :1 TRUE :1 #&gt; V33 V34 V35 V36 #&gt; Mode :logical Mode :logical Mode :logical Mode :logical #&gt; FALSE:176 FALSE:176 FALSE:176 FALSE:176 #&gt; TRUE :1 TRUE :1 TRUE :1 TRUE :1 #&gt; V37 V38 V39 V40 #&gt; Mode :logical Mode :logical Mode :logical Mode :logical #&gt; FALSE:176 FALSE:176 FALSE:176 FALSE:176 #&gt; TRUE :1 TRUE :1 TRUE :1 TRUE :1 #&gt; V41 V42 V43 V44 #&gt; Mode :logical Mode :logical Mode :logical Mode :logical #&gt; FALSE:176 FALSE:176 FALSE:176 FALSE:176 #&gt; TRUE :1 TRUE :1 TRUE :1 TRUE :1 #&gt; V45 V46 V47 V48 #&gt; Mode :logical Mode :logical Mode :logical Mode :logical #&gt; FALSE:176 FALSE:176 FALSE:176 FALSE:176 #&gt; TRUE :1 TRUE :1 TRUE :1 TRUE :1 #&gt; V49 V50 V51 #&gt; Mode :logical Mode :logical Mode :logical #&gt; FALSE:176 FALSE:176 FALSE:176 #&gt; TRUE :1 TRUE :1 TRUE :1 This shows that there are 56 countries in Africa. We can check if they are the same countries as follows: africa_centroids2 = world_centroids[sel_africa,] identical(africa_centroids, africa_centroids2) #&gt; [1] FALSE 2.3.3 Exercises Perform the same operations and map making for another continent of your choice. Bonus: Download some global geographic data and add attribute variables assigning them to the continents of the world. 2.4 Vector data 2.5 Raster data References "],
["attr.html", "3 Attribute data operations Prerequisites 3.1 Introduction 3.2 Base vs data.table vs dplyr 3.3 Attribute subsetting 3.4 Attribute data aggregation 3.5 Attribute data joining", " 3 Attribute data operations Prerequisites This chapter requires dplyr, sf and spData packages: library(sf) library(tidyverse) You must have loaded the world data from the spData package: f = system.file(&quot;shapes/wrld.shp&quot;, package = &quot;spData&quot;) world = st_read(f) 3.1 Introduction Attribute data is non-geographical information associated with geographical data. There is a strong overlap between geographical and non-geographical operations: non-spatial subset, aggregate and join operations each have their geographical equivalents (see 4). The non-spatial versions of these methods are common and easy to understand with R, so they are covered first. The methods are largely cross-transferable to the trickier tasks of spatial data operations, so pay attention! Simple features defined by the sf package make working with attribute data easy because objects of class sf are data frames. This means that all the accumulated wisdom and functions accumulated in the R community for handling data frames can be applied to the non-geographic aspects of data, as illustrated below for the world object: class(world) #&gt; [1] &quot;sf&quot; &quot;data.frame&quot; This ‘world’ dataset contains 63 non-geographical variables (and one geometry column) with data for almost 200 countries, as can be ascertained using base functions for working with tabular data: dim(world) # it is a 2 dimensional object, with rows and columns #&gt; [1] 177 11 nrow(world) # how many rows? #&gt; [1] 177 ncol(world) # how many columns? #&gt; [1] 11 Extracting the attribute data of an sf object is the same as removing the geometry column: world_df = world st_geometry(world_df) = NULL class(world_df) #&gt; [1] &quot;data.frame&quot; This can be useful if the geometry column causes problem, e.g. by occupying large amounts of RAM. However, for most cases there is no harm in keeping the geometry column, as data frame operations on sf will only act on the attribute data. For this reason, being good at working with attribute data in geographical data is the same being proficient at handling data frames in R. For many applications, the most effective and intuitive way to work with data frames is with the dplyr package. 3.2 Base vs data.table vs dplyr Simple feature objects of class sf behave exactly the same as data.frame objects for most base R operations. Unlike objects of class Spatial defined by the sp package, sf objects are also compatible with dplyr and data.table packages. This is an advantage because they provide fast functions for data manipulation. Which method you use is largely a matter of preference. In this chapter the focus is largely on dplyr because of it’s intuitive function names and its ability to perform multiple chained operations using the pipe operator. The important thing is that you select a data processing paradigm of choice, and master it. 3.3 Attribute subsetting 3.4 Attribute data aggregation 3.5 Attribute data joining as illustrated in the code examples below (results not shown). world[1:6, ] # subset rows world[, 1:3] # subset columns After each operation, the geometry column is preserved. dplyr makes working with data frames easier and is compatible with sf objects, after the package has been loaded: The select() function, for example, can be used to both subset and renames columns in a single line, for example: world_orig = world # create copy of world dataset for future reference world1 = select(world_orig, name_long, continent, population = pop) head(world1, n = 2) #&gt; Simple feature collection with 2 features and 3 fields #&gt; geometry type: MULTIPOLYGON #&gt; dimension: XY #&gt; bbox: xmin: 11.6401 ymin: -17.93064 xmax: 75.15803 ymax: 38.48628 #&gt; epsg (SRID): 4326 #&gt; proj4string: +proj=longlat +datum=WGS84 +no_defs #&gt; name_long continent population geometry #&gt; 1 Afghanistan Asia 31627506 MULTIPOLYGON(((61.210817091... #&gt; 2 Angola Africa 24227524 MULTIPOLYGON(((16.326528354... This is more concises than the base R equivalent (which saves the result as an object called world2 to avoid overriding the world dataset created previously): world2 = world_orig[c(&quot;name_long&quot;, &quot;continent&quot;, &quot;pop&quot;)] # subset columns by name names(world2)[3] = &quot;population&quot; # rename column manually The pipe operator (%&gt;%), which passes the output of one function into the first argument of the next function, is commonly used in dplyr data analysis workflows. This works because the fundamental dplyr functions (or ‘verbs’, like select()) all take a data frame object in and spit a data frame object out. Combining many functions together with pipes is called chaining or piping. The advantage over base R for complex data processing operations is that this approach prevents nested functions and is easy to read because there is a clear order and modularity to the work (a piped command can be commented out, for example). The example below shows yet another way of creating the renamed world dataset, using the pipe operator: world3 = world_orig %&gt;% select(name_long, continent) The pipe operator can be used for many data processing tasks with attribute data: # todo - describe these: ==, !=, &gt;, &gt;=, &lt;, &lt;=, &amp;, | # Filtering attribute data with dplyr world_few_rows = world %&gt;% filter(pop &gt; 1e9) head(world_few_rows) #&gt; Simple feature collection with 2 features and 10 fields #&gt; geometry type: MULTIPOLYGON #&gt; dimension: XY #&gt; bbox: xmin: 68.17665 ymin: 7.965535 xmax: 135.0263 ymax: 53.4588 #&gt; epsg (SRID): 4326 #&gt; proj4string: +proj=longlat +datum=WGS84 +no_defs #&gt; iso_a2 name_long continent region_un subregion type #&gt; 1 CN China Asia Asia Eastern Asia Country #&gt; 2 IN India Asia Asia Southern Asia Sovereign country #&gt; area_km2 pop lifeExp gdpPercap geometry #&gt; 1 9409832 1.36e+09 75.8 12759 MULTIPOLYGON(((110.33918786... #&gt; 2 3142892 1.30e+09 68.0 5392 MULTIPOLYGON(((77.837450799... This is equivalent to the following base R code (note NAs are forbidden for subsetting): # subsetting simple feature rows by values world$pop[is.na(world$pop)] = 0 world_few_rows = world[world$pop &gt; 1e9,] % --> % --> # data summary (not shown) summary(world) # data summary by groups (not shown) world_continents = world %&gt;% group_by(continent) %&gt;% summarise(continent_pop = sum(pop), country_n = n()) world_continents # sort variables ## by name world_continents %&gt;% arrange(continent) #&gt; Simple feature collection with 8 features and 3 fields #&gt; geometry type: GEOMETRY #&gt; dimension: XY #&gt; bbox: xmin: -180 ymin: -90 xmax: 180 ymax: 83.64513 #&gt; epsg (SRID): 4326 #&gt; proj4string: +proj=longlat +datum=WGS84 +no_defs #&gt; # A tibble: 8 x 4 #&gt; continent continent_pop country_n geometry #&gt; &lt;fctr&gt; &lt;dbl&gt; &lt;int&gt; &lt;simple_feature&gt; #&gt; 1 Africa NA 51 &lt;MULTIPOLYGON...&gt; #&gt; 2 Antarctica NA 1 &lt;MULTIPOLYGON...&gt; #&gt; 3 Asia NA 47 &lt;MULTIPOLYGON...&gt; #&gt; 4 Europe NA 39 &lt;MULTIPOLYGON...&gt; #&gt; # ... with 4 more rows ## by population (in descending order) world_continents %&gt;% arrange(-continent_pop) #&gt; Simple feature collection with 8 features and 3 fields #&gt; geometry type: GEOMETRY #&gt; dimension: XY #&gt; bbox: xmin: -180 ymin: -90 xmax: 180 ymax: 83.64513 #&gt; epsg (SRID): 4326 #&gt; proj4string: +proj=longlat +datum=WGS84 +no_defs #&gt; # A tibble: 8 x 4 #&gt; continent continent_pop country_n geometry #&gt; &lt;fctr&gt; &lt;dbl&gt; &lt;int&gt; &lt;simple_feature&gt; #&gt; 1 North America 5.65e+08 18 &lt;MULTIPOLYGON...&gt; #&gt; 2 Oceania 3.74e+07 7 &lt;MULTIPOLYGON...&gt; #&gt; 3 Africa NA 51 &lt;MULTIPOLYGON...&gt; #&gt; 4 Antarctica NA 1 &lt;MULTIPOLYGON...&gt; #&gt; # ... with 4 more rows Most of the function from sf package do not drop a geometry column. To extract a data frame st_geometry() or st_set_geometry() function can be used. world_st = world st_geometry(world_st) = NULL class(world_st) #&gt; [1] &quot;data.frame&quot; # OR world_st2 = world world_st2 = world_st2 %&gt;% st_set_geometry(NULL) class(world_st2) #&gt; [1] &quot;data.frame&quot; "],
["spatial-data-operations.html", "4 Spatial data operations 4.1 Attribute subsetting 4.2 Attribute data aggregation 4.3 Attribute data joining", " 4 Spatial data operations 4.1 Attribute subsetting 4.2 Attribute data aggregation 4.3 Attribute data joining "],
["read-write.html", "5 Geographical data I/O 5.1 Data Input (I) 5.2 Data output (O) 5.3 Visual outputs", " 5 Geographical data I/O The previous chapters introduced this book and provided an overview of spatial data classes in R, with a focus on simple features. This chapter is about getting spatial data onto your computer and then, perhaps after processing it with techniques described in this book, back out to the world. We include a section (5.3) on visualization because outputting data in a human (not just computer) readable enables non-programmers to benefit from your work. If your aim is to use geocomputation to improve the world, e.g. by encouraging evidence-based policies, this final stage is vital. I/O is short for “input/output” which means, in plain English, “reading and writing data”. We use the acronym instead of plain English not to confuse you or to make chapter names short, but because that’s the term used in computer science and it is useful to think of data import and export from a computing perspective.3 5.1 Data Input (I) To efficiently read data into R, it helps to have an understanding of what happens ‘under the hood’. Executing commands such as sf::st_read (the main function we use for loading spatial data, from the sf package) or readr::read_csv silently sets-off a chain of events that loads spatial objects. “Loading” in this context means loading the data into R or, more precisely, assigning objects to your workspace, stored in RAM accessible from the .GlobalEnv of your current R session. Spatial data comes in a wide variety of file formats, and sf is adept at handling them, via the command st_read. This function (also called read_sf) uses the power of the GDAL C/C++ library behind the scenes, allowing sf to read a very wide range of spatial data formats. The first arguement of st_read is file, which should be a text string or an object containing a single text string: library(sf) #&gt; Linking to GEOS 3.5.0, GDAL 2.1.0, proj.4 4.8.0 f = system.file(&quot;shapes/wrld.shp&quot;, package = &quot;spData&quot;) world = st_read(f) #&gt; Reading layer `wrld&#39; from data source `/home/travis/R/Library/spData/shapes/wrld.shp&#39; using driver `ESRI Shapefile&#39; #&gt; converted into: POLYGON #&gt; Simple feature collection with 177 features and 10 fields #&gt; geometry type: MULTIPOLYGON #&gt; dimension: XY #&gt; bbox: xmin: -180 ymin: -90 xmax: 180 ymax: 83.64513 #&gt; epsg (SRID): 4326 #&gt; proj4string: +proj=longlat +datum=WGS84 +no_defs A major advantage of sf is that it is fast at geographical data I/O, as illustrated in the benchmark below: library(microbenchmark) bench_read = microbenchmark(times = 5, st_read(f), rgdal::readOGR(f) ) bench_read$time[1] / bench_read$time[2] #&gt; [1] 2.34 The results demonstrate that sf can be much faster (2 times faster in this case) than rgdal at reading-in the world countries shapefile. The counterpart of st_read() is st_write(). This allows writing to a range of geographic vector file types, including the common formats .geojson, .shp and .gpkg. st_read() will decide which driver to use automatically, based on the file name, as illustrated in the benchmark below demonstrating write speeds for each format. system.time(st_write(world, &quot;world.geojson&quot;, quiet = TRUE)) #&gt; user system elapsed #&gt; 0.084 0.000 0.082 system.time(st_write(world, &quot;world.shp&quot;, quiet = TRUE)) #&gt; user system elapsed #&gt; 0.056 0.000 0.057 system.time(st_write(world, &quot;world.gpkg&quot;, quiet = TRUE)) #&gt; user system elapsed #&gt; 0.028 0.008 0.043 The full range of file-types supported by sf is reported by st_drivers(), the first 2 of which are shown below: sf_drivers = st_drivers() head(sf_drivers, n = 2) #&gt; name long_name write copy is_raster is_vector #&gt; PCIDSK PCIDSK PCIDSK Database File TRUE FALSE TRUE TRUE #&gt; netCDF netCDF Network Common Data Format TRUE TRUE TRUE TRUE 5.2 Data output (O) 5.3 Visual outputs Concepts such as computational efficiency, hard disk space and ‘idempotence’ are useful when thinking about reading and writing geographical datasets, which can become large and difficult to handle. Loading/saving data is yet another way of saying the same thing.↩ "],
["coord.html", "6 Coordinate systems/reprojecting Prerequisites", " 6 Coordinate systems/reprojecting Prerequisites "],
["vector.html", "7 Working with vector data Prerequisites", " 7 Working with vector data Prerequisites "],
["raster.html", "8 Working with raster data Prerequisites", " 8 Working with raster data Prerequisites "],
["raster-vector.html", "9 Raster-vector interaction Prerequisites", " 9 Raster-vector interaction Prerequisites "],
["point-pattern-analysis-and-spatial-interpolation.html", "10 Point Pattern analysis and spatial interpolation 10.1 Data 10.2 Point density 10.3 Points in polygons and raster cells 10.4 Point distance analysis 10.5 Spatial interpolation 10.6 Voronoi polygon interpolation 10.7 Interpolation with the gstat package", " 10 Point Pattern analysis and spatial interpolation This chapter teaches the basics of point pattern analysis in R. It is influenced by the chapter on Spatial Point Pattern Analysis in Applied Spatial Data Analysis with R (Bivand, Pebesma, and Gómez-Rubio 2013) and an online tutorial on Point Pattern Analyis by Robert Hijmans. We will use the sp package for this rather than the newer sf package, as point pattern analysis is more established for the former Spatial class system than the latter’s sf classes. We will also use raster as it has concise and well-designed functions for spatial data: pkgs = c( &quot;sp&quot;, &quot;tmap&quot;, &quot;raster&quot;, &quot;mapview&quot;, &quot;dismo&quot;, &quot;gstat&quot; ) i = pkgs[!pkgs %in% installed.packages()] if(length(i) &gt; 0) install.packages(i) lapply(pkgs, library, character.only = TRUE) #&gt; Loading required package: leaflet 10.1 Data This chapter uses two datasets on the spatial distribution and some attributes of cycle hire ‘docking stations’, one from OpenStreetMap and one from an on-line data feed.4 It also uses boundary data representing the 33 boroughs of London. These datasets live in the spData package and can be loaded as follows: library(spData) f = system.file(&quot;shapes/&quot;, package = &quot;spData&quot;) lnd = rgdal::readOGR(paste0(f, &quot;lnd.geojson&quot;)) #&gt; OGR data source with driver: GeoJSON #&gt; Source: &quot;/home/travis/R/Library/spData/shapes/lnd.geojson&quot;, layer: &quot;OGRGeoJSON&quot; #&gt; with 33 features #&gt; It has 7 fields cycle_hire = rgdal::readOGR(paste0(f, &quot;cycle_hire.geojson&quot;)) #&gt; OGR data source with driver: GeoJSON #&gt; Source: &quot;/home/travis/R/Library/spData/shapes/cycle_hire.geojson&quot;, layer: &quot;OGRGeoJSON&quot; #&gt; with 742 features #&gt; It has 5 fields cycle_hire_osm = rgdal::readOGR(paste0(f, &quot;cycle_hire_osm.geojson&quot;)) #&gt; OGR data source with driver: GeoJSON #&gt; Source: &quot;/home/travis/R/Library/spData/shapes/cycle_hire_osm.geojson&quot;, layer: &quot;OGRGeoJSON&quot; #&gt; with 532 features #&gt; It has 5 fields We use the Spatial classes used by the sp dataset, as these are required by point pattern analysis functions used in the chapter. This, and a basic plot of the data, is done in the code chunk below (note the similarities with sf plots): library(sp) plot(cycle_hire) points(cycle_hire_osm, col = &quot;red&quot;) plot(lnd, add = TRUE) It is immediately clear that the two datasets on cycle hire points are closely related (they have a high degree of spatial correlation) and have a distinctive pattern. cycle_hire represents official data on cycle parking, and will be the main point dataset analysed. cycle_hire_osm is the community contributed dataset on cycle hire locations, downloaded from OpenStreetMap. Both sets of points overlay some of London’s 33 boroughs, the central ones, and seem to follow the River Thames, especially along the north bank of the river. But how to describe that information quantitatively, and extrapolate the values from the plotted location to other areas? It is the purpose of this chapter to provide the know-how to answer such questions, that should be extensible to many applications that involve point data. 10.2 Point density A basic statistic to compute on points within a polygon is the number of points per polygon, and the related statistic of point density. Let’s first compute that for London overall, before doing a zone-by-zone breakdown: nrow(cycle_hire) #&gt; [1] 742 lnd_area = sum(area(lnd)) / 1e6 The results show that there are 742 cycle hire points and that London covers an area of just over one and a half thousand square kilometres (1 km2 = 1000000 m2 = 1e6 m2 in scientific notation). That represents on average roughly one cycle parking rental space per 2 square kilometers, or half a rental point per square kilometer, as revealed by the results of the calculation below: nrow(cycle_hire) / lnd_area #&gt; [1] 0.471 This is not a good indicator of the density of the bike hire scheme overall, because they are so concentrated in central London. A more representative result can be found by calculating the average point density within the extent of the bike hire scheme. We can coerce the bounding box (or extent in raster terminology) of the stations into a polygon whose area can be measured with the following commands: bb_hire = as(extent(cycle_hire), &quot;SpatialPolygons&quot;) crs(bb_hire) = crs(lnd) c_area = area(bb_hire) / 1e6 c_area #&gt; [1] 158 10.2.1 Exercises What is the average point density of cycle hire points within the scheme’s bounding box? Why did we add the second line of code in the previous code chunk? Why are there two crs() calls? The above chunk uses raster functions. How would you write the above code using sp code? 10.2.2 Challenges Reproduce the result using sp code. Reproduce the results using sf code. 10.3 Points in polygons and raster cells A useful level of analysis at which to analyse the geographical distribution of points is the zone-level. We can aggregate the points per zone and provide summary statistics. Starting with the number of points per polygon, this would calculated as follows: crs(lnd) = crs(cycle_hire) cycle_hire_ag = aggregate(cycle_hire[&quot;id&quot;], lnd, FUN = &quot;length&quot;) 10.3.1 Exercises Based on an analysis of the cycle_hire_ag: How many zones contain no cycle hire points? What is the average number of cycle hire points per zone? 10.3.2 Challenge Find the average density of cycle hire points per zone in London. Plot the result in an attractive map (e.g. as shown below). Find which zone has the highest density of cycle hire points. A problem with the zonal representation of point density is that the results are dependent on the somewhat arbitrary shapes and sizes of the zones in which the points are aggregated. To overcome this problem we can create a raster representation of the points: r = raster(bb_hire, ncol = 16, nrow = 10) rc = rasterize(cycle_hire@coords, r, fun = &quot;count&quot;) plot(rc) points(cycle_hire) plot(lnd, add = TRUE) This is already very useful. The results show that there are 5 clusters of cycle parking with much higher density than the surrounding areas. We can visualise these clusters using a static plot as follows: plot(rc) plot(rc &gt; 12) More useful, in terms of characterising the geographical characteristics of each cluster, would be to plot these 5 clusters interactively. Do this with mapview: library(mapview) mapview(rc &gt; 12) + mapview(cycle_hire) The resulting interactive plot draws attention to the areas of high point density, such as the area surrounding Victoria station, illustrated below. 10.3.3 Exercises Explore the interactive map created by mapview above. Try to explain the location of the high density clusters: what are they near? Where would you suggest building more cycle hire points? 10.4 Point distance analysis Another important characteristic of point patterns is the distances between points, which can be calculated using raster’s dist() function: d = spDists(cycle_hire, longlat = TRUE) dm = as.matrix(d) dm[1:3, 1:5] #&gt; [,1] [,2] [,3] [,4] [,5] #&gt; [1,] 0.00 6.91 1.97 0.77 5.16 #&gt; [2,] 6.91 0.00 8.21 6.31 2.92 #&gt; [3,] 1.97 8.21 0.00 2.71 5.92 The results show the distance, in km, form every point to every other. The dm object is known as a distance matrix: note the diagonal of zero values. This distance matrix is very useful for various types of analysis, a couple of which we’ll explore below. To find the minimum distance of each point to every other, we can use the apply function, for each row, and then select the top 5: diag(dm) = NA dmin = apply(X = dm, MARGIN = 1, FUN = min, na.rm = TRUE) sel_isolated = order(dmin, decreasing = TRUE)[1:5] qtm(cycle_hire, col = &quot;grey&quot;, main = &quot;Isolated points&quot;) + qtm(cycle_hire[sel_isolated,], symbols.col = &quot;red&quot;, symbols.size = 2) + tm_scale_bar() Another plot that is useful is that of the ‘G function’ for exploring the extent to which points cluster or separate compared with what would be expected from a random distribution (Bivand, Pebesma, and Gómez-Rubio 2013): distance = sort(unique(round(dmin, digits = 3))) Gd = sapply(distance, function(x) sum(dmin &lt; x)) Gd = Gd / length(dmin) plot(distance, Gd) 10.5 Spatial interpolation Spatial interpolation refers to methods of estimating the value of something in one place, based on measurements taken elsewhere. It depends on spatial autocorrelation, defined in Waldo Tobler’s ‘first law of Geography’ as follows (Miller 2004): Everything is related to everything else, but near things are more related than distant thing Building on the example of cycle hire points in London, we can ask the question: what is the expected number of bikes for a stand in location x, given knowledge of the existing data. Thus spatial interpolation requires a dependent variable, which is summarised numerically and visually below: summary(cycle_hire$nbikes) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 0.0 3.0 11.0 12.2 19.0 51.0 tm_shape(cycle_hire) + tm_symbols(col = &quot;nbikes&quot;, palette = &quot;YlOrRd&quot;, alpha = 0.6, style = &quot;quantile&quot;) There is a clear spatial pattern to this: there are more bikes parked in the outer docking stations. We can say that verbally, but how to we represent that on the map? A first port of call would be to rasterise the result, using the the raster representation of the study area contained in the object r to find the mean per cell: rnbikes = rasterize(cycle_hire, r, field = &quot;nbikes&quot;, fun = mean) plot(rnbikes) What about estimating the values of cells outside the current network area? We can use raster’s focal() function to estimate that. w = matrix(1, nc = 9, nr = 9) r_interp1 = focal(x = rnbikes, w = w, fun = mean, NAonly = TRUE, na.rm = TRUE, pad = TRUE) plot(r_interp1) points(cycle_hire) 10.5.1 Exercises Experiment with different matrix sizes of w in the above code block. What difference does the size make? Note that the 9x9 cell focal point leads to an ‘over smoothing’ of the results. Find a way to include only values from touching cells in the results. 10.6 Voronoi polygon interpolation The raster cell method of spatial interpolation was fun, but not that sophisticated or spatially precise, with a resolution of around 1 km. The next simplest solution is to break the area up into pieces and assign the value of the entire area to the value of the point it contains: library(dismo) v = voronoi(cycle_hire) v = intersect(v, r) #&gt; Warning in intersect(x, y): non identical CRS tm_shape(v) + tm_fill(&quot;nbikes&quot;, palette = &quot;YlOrRd&quot;, style = &quot;quantile&quot;) + qtm(cycle_hire, symbols.size = 0.2) 10.6.1 Exercises Create a point at a random location on the map and plot it prominently on top of the previously plotted layers. What would be it’s estimated ‘nbikes’ a) from the voronoi polygon interpolation and b) from the raster interpolation. Which do you think is most accurate? 10.7 Interpolation with the gstat package gstat provides a number of functions for spatial prediction and interpolation using a range of models. The most basic of these, and a workhorse for spatial interpolation is Inverse Distance Weighting (IDW): library(gstat) gs = gstat(formula = nbikes~1, locations = cycle_hire) crs(r) = crs(lnd) r_idw = interpolate(r, gs) #&gt; [inverse distance weighted interpolation] plot(r_idw) 10.7.1 Exercises Look at the original data - what could explain the spatial distribution of the nbikes variable? Experiment with the spatial resolution - we’re using 1 km grid cells which are huge! Try using other methods described in Robert Hijman’s tutorial on spatial interpolation. Try cross-validating the results. Which performs best? References "],
["references.html", "11 References", " 11 References "]
]
